{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2dBhf42ojz9EbHAI0C7Be",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielJobert/EU-27-trade-network-analysis/blob/main/M%C3%A9moire_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of libraries to install\n",
        "libraries = ['torch', 'transformers', 'datasets', 'torchtext', 'faiss-cpu']\n",
        "\n",
        "for lib in libraries:\n",
        "    try:\n",
        "        # Attempt to install the library\n",
        "        !pip install {lib}\n",
        "        print(f'Successfully installed {lib}')\n",
        "    except Exception as e:\n",
        "        # Catch and print any errors\n",
        "        print(f'An error occurred while installing {lib}: {e}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4St723nNHdmz",
        "outputId": "3f00d1b4-d4c5-4b78-e0b6-3affbe5194d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Successfully installed torch\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Successfully installed transformers\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Successfully installed datasets\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Successfully installed torchtext\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Successfully installed faiss-cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE1OA2N42VTC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class RAGDatasetHandler(Dataset):\n",
        "    \"\"\"\n",
        "    Class to handle loading, preprocessing, and tokenization of various datasets for RAG model training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_name: str, version: str = None, split: str = 'train',\n",
        "                 tokenizer_name: str = 'bert-base-uncased', max_length: int = 384,\n",
        "                 include_negatives: bool = False):\n",
        "        \"\"\"\n",
        "        Initializes the RAGDatasetHandler with dataset name, split, tokenizer, and preprocessing options.\n",
        "\n",
        "        Args:\n",
        "            dataset_name (str): Name of the dataset (e.g., 'squad').\n",
        "            version (str, optional): Specific version of the dataset (e.g., '2.0' for SQuAD 2.0).\n",
        "            split (str, optional): Dataset split to load ('train', 'validation', etc.).\n",
        "            tokenizer_name (str, optional): Name of the tokenizer to use (e.g., 'bert-base-uncased').\n",
        "            max_length (int, optional): Maximum length for tokenized inputs.\n",
        "            include_negatives (bool, optional): Whether to include negative samples.\n",
        "        \"\"\"\n",
        "        self.dataset_name = dataset_name\n",
        "        self.version = version\n",
        "        self.split = split\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.max_length = max_length\n",
        "        self.include_negatives = include_negatives\n",
        "        self.dataset = self._load_dataset()\n",
        "\n",
        "    def _load_dataset(self):\n",
        "        \"\"\"\n",
        "        Loads the specified dataset using the Hugging Face datasets library.\n",
        "\n",
        "        Returns:\n",
        "            Dataset: The loaded dataset.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the dataset name is not recognized.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.dataset_name == 'squad' and self.version == '2.0':\n",
        "                return load_dataset('squad_v2', split=self.split)\n",
        "            elif self.dataset_name == 'squad':\n",
        "                return load_dataset('squad', split=self.split)\n",
        "            else:\n",
        "                # Implement loading for other datasets here as needed\n",
        "                raise ValueError(f\"Unsupported dataset: {self.dataset_name} version: {self.version}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}\")\n",
        "            raise\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves an item from the dataset, tokenizes the context and question,\n",
        "        and prepares data for the model. Optionally includes negative sampling.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing tokenized inputs for the model.\n",
        "        \"\"\"\n",
        "        # Extract data at the specified index\n",
        "        data = self.dataset[idx]\n",
        "\n",
        "        # Tokenize the context (passage)\n",
        "        passage_inputs = self.tokenizer(\n",
        "            data['context'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Tokenize the query (question)\n",
        "        query_inputs = self.tokenizer(\n",
        "            data['question'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Convert to single tensors\n",
        "        passage_input_ids = passage_inputs['input_ids'].squeeze()\n",
        "        passage_attention_mask = passage_inputs['attention_mask'].squeeze()\n",
        "\n",
        "        query_input_ids = query_inputs['input_ids'].squeeze()\n",
        "        query_attention_mask = query_inputs['attention_mask'].squeeze()\n",
        "\n",
        "        # If including negatives, prepare a negative sample\n",
        "        if self.include_negatives:\n",
        "            # Find a random negative passage (simple implementation, could be more sophisticated)\n",
        "            neg_idx = (idx + 1) % len(self.dataset)\n",
        "            negative_data = self.dataset[neg_idx]\n",
        "            negative_inputs = self.tokenizer(\n",
        "                negative_data['context'],\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            negative_input_ids = negative_inputs['input_ids'].squeeze()\n",
        "            negative_attention_mask = negative_inputs['attention_mask'].squeeze()\n",
        "\n",
        "            return {\n",
        "                'query_input_ids': query_input_ids,\n",
        "                'query_attention_mask': query_attention_mask,\n",
        "                'passage_input_ids': passage_input_ids,\n",
        "                'passage_attention_mask': passage_attention_mask,\n",
        "                'negative_input_ids': negative_input_ids,\n",
        "                'negative_attention_mask': negative_attention_mask\n",
        "            }\n",
        "\n",
        "        # Return positive pairs\n",
        "        return {\n",
        "            'query_input_ids': query_input_ids,\n",
        "            'query_attention_mask': query_attention_mask,\n",
        "            'passage_input_ids': passage_input_ids,\n",
        "            'passage_attention_mask': passage_attention_mask\n",
        "        }\n",
        "\n",
        "    def get_dataloader(self, batch_size: int = 8, shuffle: bool = True):\n",
        "        \"\"\"\n",
        "        Creates a DataLoader for the dataset.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int, optional): Number of samples per batch.\n",
        "            shuffle (bool, optional): Whether to shuffle the data.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: A DataLoader instance for the dataset.\n",
        "        \"\"\"\n",
        "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    rag_dataset = RAGDatasetHandler(dataset_name='squad', version='2.0', split='train', include_negatives=True)\n",
        "    dataloader = rag_dataset.get_dataloader(batch_size=8, shuffle=True)\n",
        "\n",
        "    # Iterate over the DataLoader\n",
        "    for batch in dataloader:\n",
        "        print(batch)\n",
        "        break\n"
      ],
      "metadata": {
        "id": "9srgtW3b9KGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31644c5c-3958-4860-d356-f5fbb6a8ffa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query_input_ids': tensor([[ 101, 2043, 2106,  ...,    0,    0,    0],\n",
            "        [ 101, 2044, 3930,  ...,    0,    0,    0],\n",
            "        [ 101, 2054, 3139,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2054, 2001,  ...,    0,    0,    0],\n",
            "        [ 101, 2976, 2375,  ...,    0,    0,    0],\n",
            "        [ 101, 2339, 2001,  ...,    0,    0,    0]]), 'query_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'passage_input_ids': tensor([[ 101, 2045, 2003,  ...,    0,    0,    0],\n",
            "        [ 101, 3078, 3348,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 3470,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 7862, 2031,  ...,    0,    0,    0],\n",
            "        [ 101, 2976, 2375,  ...,    0,    0,    0],\n",
            "        [ 101, 2750, 3278,  ...,    0,    0,    0]]), 'passage_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'negative_input_ids': tensor([[ 101, 2045, 2003,  ...,    0,    0,    0],\n",
            "        [ 101, 3078, 3348,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 3470,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 7862, 2031,  ...,    0,    0,    0],\n",
            "        [ 101, 2976, 2375,  ...,    0,    0,    0],\n",
            "        [ 101, 2750, 3278,  ...,    0,    0,    0]]), 'negative_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "import faiss\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "\n",
        "class Retriever(ABC):\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        if self.device == 'cuda':\n",
        "            self.model = self.model.half()\n",
        "\n",
        "    @abstractmethod\n",
        "    def encode_passages(self, passages):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def retrieve(self, query, index, passages, top_k):\n",
        "        pass\n",
        "\n",
        "class LanguageModel(ABC):\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "        if self.device == 'cuda':\n",
        "            self.model = self.model.half()\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, input_text):\n",
        "        pass\n",
        "\n",
        "class ContrieverRetriever(Retriever):\n",
        "    def encode_passages(self, passages, batch_size=16):\n",
        "        embeddings = []\n",
        "        for i in range(0, len(passages), batch_size):\n",
        "            batch = passages[i:i + batch_size]\n",
        "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
        "            with torch.no_grad():\n",
        "                batch_embeddings = self.model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "            embeddings.append(batch_embeddings)\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def retrieve(self, query, index, passages, top_k=5):\n",
        "        query_inputs = self.tokenizer(query, return_tensors='pt').to(self.device)\n",
        "        with torch.no_grad():\n",
        "            query_embedding = self.model(**query_inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        distances, indices = index.search(query_embedding, top_k)\n",
        "        return [passages[i] for i in indices[0]]\n",
        "\n",
        "class FlanT5LanguageModel(LanguageModel):\n",
        "    def generate(self, input_text):\n",
        "        input_ids = self.tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512).input_ids.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(input_ids, max_length=256)\n",
        "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self, retriever, language_model, passages, index_file='passage_index.faiss'):\n",
        "        self.retriever = retriever\n",
        "        self.language_model = language_model\n",
        "        self.passages = passages\n",
        "        self.index_file = index_file\n",
        "        self.index = self._load_index() or self._build_index(passages)\n",
        "\n",
        "    def _build_index(self, passages, batch_size=16):\n",
        "        passage_embeddings = self.retriever.encode_passages(passages, batch_size)\n",
        "        dimension = passage_embeddings.shape[1]\n",
        "        index = faiss.IndexFlatIP(dimension)\n",
        "        index.add(passage_embeddings)\n",
        "        faiss.write_index(index, self.index_file)\n",
        "        return index\n",
        "\n",
        "    def _load_index(self):\n",
        "        try:\n",
        "            return faiss.read_index(self.index_file)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def __call__(self, query, top_k=5):\n",
        "        retrieved_passages = self.retriever.retrieve(query, self.index, self.passages, top_k)\n",
        "        input_text = \" \".join(retrieved_passages[:top_k]) + \" \" + query\n",
        "        answer = self.language_model.generate(input_text)\n",
        "        return answer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load a subset of the dataset with only 10 items\n",
        "    dataset = load_dataset('squad_v2', split='train[:10]')\n",
        "    passages = [item['context'] for item in dataset]\n",
        "\n",
        "    # Instantiate retriever and language model\n",
        "    retriever = ContrieverRetriever(model_name='facebook/contriever')\n",
        "    language_model = FlanT5LanguageModel(model_name='google/flan-t5-base')\n",
        "\n",
        "    # Create the RAG pipeline\n",
        "    pipeline = RAGPipeline(retriever=retriever, language_model=language_model, passages=passages)\n",
        "\n",
        "    # Test the pipeline\n",
        "    query = \"What is the capital of France?\"\n",
        "    result = pipeline(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Generated Answer: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p36Rshgth8EF",
        "outputId": "d80bf561-0972-495e-c179-f45423b8bafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the capital of France?\n",
            "Generated Answer: -selling girl groups of all time.\n"
          ]
        }
      ]
    }
  ]
}